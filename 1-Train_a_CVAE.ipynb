{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "  <hr> <center> <font size=\"+3.5\"> <b> Interpréter des conditions atypiques à l'aide d'autoencodeurs variationnels conditionnels </b> </font> </center> <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block\" role=\"alert\">\n",
    "    <center><b> <u>Auteur :</u>  Clement GOUBET  </b></center>\n",
    "</div>\n",
    "<div class=\"alert alert-block\" role=\"alert\">\n",
    "    <center> <font size=\"+1.5\"> <b>  23 juillet 2019  </b>  </font> </center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table de contenu\n",
    "\n",
    "## 0. Préparation de l'environnement de travail\n",
    "- chargement des modules\n",
    "- mise en forme des données\n",
    "- définition de quelques fonctions utiles\n",
    "\n",
    "## 1. Premier modèle VAE\n",
    "- construction du modèle\n",
    "- entrainement\n",
    "- évaluation de la représentation obtenue\n",
    "- premières interprétations\n",
    "\n",
    "## 2. Modèle CVAE\n",
    "- construction du modèle\n",
    "- entrainement\n",
    "- évaluation de la représentation obtenue\n",
    "- premières interprétations\n",
    "- analyse spécifique jours fériés et points atypiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <font size=\"+2\"> <b> 0. Preparation de l'environnement de travail </b> </font> <hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des modules et mise en forme des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import external libraries\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "from scipy import stats\n",
    "import cv2 #from open-cv, to convert array to images\n",
    "from IPython.display import Image\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root git folder \n",
    "name_model=\"vae_test\"\n",
    "path_second_folder = %pwd #\"/home/goubetcle/Documents/Git/VAE/\"\n",
    "save_path = os.path.join(path_second_folder,\"Results\")#\"/home/goubetcle/Documents/VAE/\"\n",
    "log_dir_model=os.path.join(save_path,\"logs\")\n",
    "#path_main_folder = '/home/jovyan'#specify the root folder of the git repo\n",
    "\n",
    "#add  to path root git folder \n",
    "sys.path.append(path_second_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "sys.path.append(path_second_folder+'/src')\n",
    "\n",
    "#import models, loss_class, params_class,AE_blocks,metrics\n",
    "from src.models import *\n",
    "#importlib.reload(models)\n",
    "from src.loss_class import *\n",
    "#importlib.reload(loss_class)\n",
    "from src.params_class import *\n",
    "#importlib.reload(params_class)\n",
    "import src.AE_blocks\n",
    "#importlib.reload(AE_blocks)\n",
    "from src.metrics import *\n",
    "#importlib.reload(metrics)\n",
    "from src.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données de consommation et de température sont des prises de mesure par pas de temps 30 minutes pendant 5 années de décembre 2012 à décembre 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "dataset_csv = os.path.join('data', \"data_conso_2012-2021.parquet.brotli\")\n",
    "df_data = pd.read_parquet(dataset_csv)\n",
    "df_data.utc_datetime = pd.to_datetime(df_data.utc_datetime, utc=True)\n",
    "\n",
    "#Visualize data frame head\n",
    "df_data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'intéresse ici à caractériser les profils journaliers. L'ensemble de données est donc transformé pour que notre jeu d'entrée soit ait en colonnes les points de mesure journaliers de la consommation d'électricité. Avant cela, tout ce qui va être inséré comme entrées du modèle est ici normalisé sur l'ensemble des mesures, puis mis en profils journaliers.\n",
    "\n",
    "Notre jeu d'entrainement et pour référence est le jeu de données 2012-2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame({\"days\" : df_data.utc_datetime.dt.date, \"minute\":df_data.utc_datetime.dt.minute+60*df_data.utc_datetime.dt.hour})\n",
    "\n",
    "df_conso, df_temp, df_prevision = make_chronics(df=pd.concat([df_data, ds], axis=1),\n",
    "                                               toshape_columns=[\"Consommation\", \"prevision_temp\", \"prevision_j-1\"],\n",
    "                                               pivot_indexcol=\"days\", pivot_columncol=\"minute\")\n",
    "\n",
    "df_conso.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conso, conso_scaler = apply_scaler(df_data, column=\"Consommation\", df_chronic=df_conso,\n",
    "                                      reference_window=df_data.utc_datetime.dt.year <=2018)\n",
    "df_conso.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'df_conso' est constitué des profils journaliers de consommation d'lélectricité que l'on va mettre en entrée de notre VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour interpréter notre représentation, on peut chercher à visualiser des paramètres connus associés à nos profils journaliers. Ici il s'agit notamment de données calendaires (mois, weekend, jour férié), auxquels on peut rajouter la température moyenne observée sur la journée (par exemple), ou encore la pente moyenne du profil.\n",
    "\n",
    "Ces informations sont à passer dans calendar_info pour être visualisés dans une projection Tensorboard, et à passer en type et en valeur respectivement dans le dictionnaire factorDesc et factorMatrix pour servir dans l'évaluation des latents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importation des données calendaires\n",
    "df_calendar = make_df_calendar(pd.DataFrame({\"ds\" : pd.to_datetime(np.asarray(df_conso.index))}))\n",
    "\n",
    "df_holidays = pd.concat([df_data[[\"is_holidays\"]],pd.DataFrame({\"ds\" : pd.to_datetime(ds.days.values)})], axis=1).drop_duplicates(\n",
    "                                               subset=\"ds\").reset_index(drop= True)\n",
    "\n",
    "df_calendar = df_calendar.merge(df_holidays, on=\"ds\", how=\"left\").rename(columns={\"is_holidays\":\"is_holiday_day\"})\n",
    "\n",
    "df_calendar.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicit the potential bridge days taken as extended holidays\n",
    "day_hol = df_calendar[['weekday', 'is_holiday_day']].copy().values\n",
    "bridge_index=[]\n",
    "for i in range(day_hol.shape[0]):\n",
    "    if day_hol[i,1]==1:\n",
    "        if day_hol[i,0]==1:\n",
    "            bridge_index.append(i-1)\n",
    "        elif day_hol[i,0]==3:\n",
    "            bridge_index.append(i+1)\n",
    "\n",
    "bridges = np.zeros(day_hol.shape[0])\n",
    "bridges[np.asarray(bridge_index)] = 1\n",
    "\n",
    "df_calendar['potential_bridge_holiday'] = bridges\n",
    "#calendar_info['potential_bridge_holiday'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_factors = [\"weekday\", \"is_weekend\", \"month\", \"is_holiday_day\"]\n",
    "factors = df_calendar[calendar_factors].copy()\n",
    "factorDesc = {ff : 'category' for ff in calendar_factors}\n",
    "\n",
    "temperatureMean= df_temp.mean(axis=1).values.reshape(-1,1)\n",
    "factorMatrix = np.c_[factors.values,temperatureMean]\n",
    "factorDesc['temperature']='regressor'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools to evaluate the representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, sin\n",
    "#visualisation d'une représentation 2D dans le notebook\n",
    "def prez_2D(x_encoded, factorMatrix, temperatureMean):\n",
    "    proj2D = PCA(2)\n",
    "    proj = proj2D.fit_transform(x_encoded)\n",
    "    plt.figure(figsize=(36,18))\n",
    "    \n",
    "    #visualisation des mois par un disque de fond coloré\n",
    "    for i in np.unique(factorMatrix[:,0]):\n",
    "        i = int(i)\n",
    "        index = factorMatrix[:,0]==i\n",
    "        plt.scatter(x=proj[index,0], y=proj[index,1], c= factorMatrix[index,2], marker = 'o', s=500, alpha=0.5, cmap = 'Paired')\n",
    "\n",
    "    plt.colorbar().set_label('month');\n",
    "    \n",
    "    #visualisation de la température par la coloration graduelle du jour de la semaine\n",
    "    for i in np.unique(factorMatrix[:,0]):\n",
    "        i = int(i)\n",
    "        index = factorMatrix[:,0]==i\n",
    "        plt.scatter(x=proj[index,0], y=proj[index,1], c= temperatureMean[index].ravel(), marker = '$'+str(i)+'$', s=200)\n",
    "\n",
    "    plt.colorbar().set_label('temperature');\n",
    "    \n",
    "def prez_3D(x_encoded, factorMatrix, temperatureMean):\n",
    "    proj3D = PCA(3)\n",
    "    proj_z = proj3D.fit_transform(x_encoded)\n",
    "    plt.figure(figsize=(36,18))\n",
    "    \n",
    "    proj = proj_z[:,:2]\n",
    "    third = proj_z[:,2]\n",
    "    \n",
    "    proj = proj + np.tile(third.reshape(-1,1), (1,2)) * np.tile(np.array([[-cos(0.785),sin(0.785)]]),(nPoints,1))\n",
    "    \n",
    "    size = 300  + (1.- np.exp(-third / max(third))) * 800\n",
    "    #visualisation des mois par un disque de fond coloré\n",
    "    for i in np.unique(factorMatrix[:,1]):\n",
    "        i = int(i)\n",
    "        index = factorMatrix[:,0]==i\n",
    "        plt.scatter(x=proj[index,0], y=proj[index,1], c= factorMatrix[index,2], marker = 'o', s=size, alpha=0.5, cmap = 'Paired')\n",
    "\n",
    "    plt.colorbar().set_label('month');\n",
    "    \n",
    "    #visualisation de la température par la coloration graduelle du jour de la semaine\n",
    "    for i in np.unique(factorMatrix[:,1]):\n",
    "        i = int(i)\n",
    "        index = factorMatrix[:,0]==i\n",
    "        plt.scatter(x=proj[index,0], y=proj[index,1], c= temperatureMean[index], marker = '$'+str(i)+'$', s=200)\n",
    "\n",
    "    plt.colorbar().set_label('temperature');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour créer une représentation sous tensorboard, la fonction suivante regroupe toutes les étapes. Le booléen includeConsuptionProfileImages active ou non le marqueur des points à l'image des profils de consommation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorboardProjection(name_model,x, x_hat, x_encoded, x_conso, calendar_info, log_dir_projector, includeConsumptionProfileImages=True):\n",
    "    #can take a bit longer to create and load in tensorboard projector, but it looks better in the projector\n",
    "    if includeConsumptionProfileImages:\n",
    "        nPoints=1500 #if you want to visualize images of consumption profiles and its recontruction in tensorboard, there is a maximum size that can be handle for a sprite image. 1830 is  \n",
    "        x_encoded_reduced=x_encoded[0:nPoints,]\n",
    "        images=createLoadProfileImages(x,x_hat,nPoints)\n",
    "    else:\n",
    "        nPoints=df_conso.shape[0]\n",
    "        \n",
    "    if includeConsumptionProfileImages:\n",
    "        sprites=images_to_sprite(images)\n",
    "        cv2.imwrite(os.path.join(log_dir_projector, 'sprite_4_classes.png'), sprites)\n",
    "    \n",
    "    writeMetaData(log_dir_projector,x_conso,calendar_info,nPoints,has_Odd=False)\n",
    "    if includeConsumptionProfileImages:\n",
    "        buildProjector(x_encoded_reduced,images=images, log_dir=log_dir_projector)\n",
    "    else:\n",
    "        buildProjector(x_encoded,images=None, log_dir=log_dir_projector)\n",
    "        \n",
    "    print(log_dir_projector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation des courbes de charge en entrée et reconstruite par le modèle\n",
    "\n",
    "def display_loads(x, x_hat,date,mae):\n",
    "    fig = plt.figure(dpi=100,figsize=(5,5))\n",
    "    #set(gca,'Color','k')\n",
    "    plt.plot(scaler_conso.inverse_transform(x), label = 'truth')\n",
    "    plt.plot(scaler_conso.inverse_transform(x_hat), '--', label = 'reconstruction')\n",
    "    plt.title(date+' - mean absolute error %0.2f GW' %(mae*std_sc/1000))\n",
    "    plt.xlabel('hours')\n",
    "    plt.ylabel('load (MW)')\n",
    "    plt.xlim((0,48))\n",
    "    plt.xticks([5,11,17,23,29,35,41], [3,6,9,12,15,18,21])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <font size=\"+2\"> <b> 1. Un modèle classique </b> </font> <hr>\n",
    "</div>\n",
    "\n",
    "## Rappels\n",
    "\n",
    "Les autoencodeurs variationnels sont des modèles qui fonctionnent en duo:\n",
    "- un encodeur qui traduit les données d'entrée dans un espace latent de dimension arbitrairement choisie. Plus exactement il cherche les paramètres de distribution des variables latentes dans chacune des dimensions.\n",
    "- un decodeur qui cherche à reconstruire les entrées à partir des coordonnées dans l'espace latent\n",
    "\n",
    "Pendant l'appentissage, le lien entre encodeur et decodeur est réalisé par l'échantillage selon les paramètres de distribution appris : chaque échantillon a des coordonnées latentes tirées aléatoirement selon cette dernière, que le décodeur cherche à retraduire dans l'espace de dimension initial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construire et entrainer un modèle VAE ou CVAE\n",
    "\n",
    "Il s'agit dans un premier temps de paramétrer les dimensions des couches de nos réseaux de neurone:\n",
    "- celles de l'encodeur e_dim\n",
    "- celles du décodeur d_dims\n",
    "- les dimensions de notre espace latent z_dim (chaque paramètre de distribution sera donc aussi de dimension z_dim)\n",
    "- les dimensions de notre embedding des conditions dans le cas dans CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de l'autoencodeur\n",
    "z_dim = 4 # le nombre de dimensions voulues dans notre représentation latente\n",
    "e_dims=[48,48,24,12]# les couches cachées du bloc encodeur; premier nombre = inputs_dim\n",
    "d_dims=[48,24,12]# les couches cachées du bloc decodeur; premier nombre = outputs_dim\n",
    "lr=3e-4 # le learning rate de l'optimiseur\n",
    "input_dim = df_conso.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les paramètres liés au modèle, il faut rappeler le fonctionnement du VAE. On cherche la maximisation de la borne inférieure de la log-vraissemblance du modèle reconstruit ou ELBO qui se décompose en deux termes :\n",
    "\n",
    "ELBO $= \\mathbb{E}_{q_\\phi(z|x)} \\log(p_\\theta(x|z)) - KL(q_\\phi(z|x) \\| p(z))$\n",
    "\n",
    "Dans notre objectif d'apprentissage, cela est traduit par une perte à minimiser de la forme:\n",
    "\n",
    "Obj $= \\|x - \\hat{x}\\| +  KL(q_\\phi(z|x) \\| p(z))$.\n",
    "\n",
    "- L'erreur de reconstruction peut être choisie selon une mesure en norme L1 ou L2. La norme L1 permet notamment d'obtenir des profils avec moins de valeurs abérrantes.\n",
    "\n",
    "- Un prior $p(z)$ laplacien plutôt que gaussien permet des concentrations plus piquées des groupements similaires, et renvoie plus loin les points atypiques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Par ailleurs, pour faciliter l'apprentissage du modèle de façon structurée dans les dimensions choisies de l'espace latent, l'action peut se porter sur le choix du prior comme sur des critères de régularisation devant le terme de divergence.\n",
    "\n",
    "    Ces termes de régularisation s'appliquent de la façon suivante :\n",
    "\n",
    "    Obj $= \\|x - \\hat{x}\\| +  \\beta KL(q_\\phi(z|x) \\| p(z)) + \\gamma MMD(q_\\phi(z) \\| p(z))$ si modèle InfoVAE\n",
    "    avec $\\beta \\in \\left[0,1\\right] $ et $\\gamma \\ge 0$ (si possible grand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de l'entrainement du modèle\n",
    "epochs = 800\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va mettre en forme ici les facteurs sur lesquels on va rendre la représentation latente agnostique. Attention à normaliser les variables quantitatives et à one-hot encoder les variables catégorielles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on va conditionner sur les profils de température et le mois\n",
    "calendar_cond = [\"month\"]\n",
    "condtovae_dims = []\n",
    "condarray=[]\n",
    "\n",
    "df_temp, _ = apply_scaler(df_data, column=\"prevision_temp\", df_chronic=df_temp,\n",
    "                                      reference_window=None)\n",
    "\n",
    "condarray.append(df_temp.values)\n",
    "condtovae_dims.append(condarray[-1].shape[1])\n",
    "\n",
    "for cc in calendar_cond:\n",
    "    condarray.append(pd.get_dummies(df_calendar[cc], prefix=cc).values)\n",
    "    condtovae_dims.append(condarray[-1].shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae_params = VAE_params(name=name_model, out_dir = log_dir_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_params.set_model_params()\n",
    "vae_params.set_training_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_params.model_params.encoder_dims = e_dims\n",
    "vae_params.model_params.decoder_dims = d_dims\n",
    "vae_params.model_params.cond_dims = condtovae_dims\n",
    "vae_params.model_params.with_embedding = True\n",
    "vae_params.model_params.emb_dims = [[48,8],[12,8],6]\n",
    "vae_params.model_params.nb_encoder_ensemble = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of custom loss definition\n",
    "def loss_1(y_true, y_pred, latent_mu ,eps=2):\n",
    "    return tf.math.reduce_sum(K.abs(K.sqrt(K.sum(K.square(latent_mu),axis=-1)) - eps))\n",
    "\n",
    "custom_loss={\"test_0\" : {\"function\" : loss_1,\n",
    "            \"args\" :{\"latent_mu\":\"kwargs['latent_components'][0]\"} }}\n",
    "\n",
    "loss_weights = {\"recon_loss\" : 1,\n",
    "                \"kl_loss\" : 0.3,\n",
    "                \"info_loss\" : 42,\n",
    "               \"test_0\":1}\n",
    "\n",
    "vae_loss = VAELoss(loss_weights = loss_weights, custom_loss=None) #change None to custom_loss if will to use one\n",
    "vae_loss.options[\"log_prior_sigma\"] = [-2.,-1.,0.,1]\n",
    "\n",
    "vae_params.training_params.loss = vae_loss\n",
    "vae_params.training_params.lr = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cae=CVAE(vae_params)\n",
    "new_cae.VAE_params.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_cae.build_model(vae_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "earl_stop = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=100, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set = df_calendar.year.values <= 2018\n",
    "test_set = df_calendar.year.values == 2019\n",
    "\n",
    "input_cvae = [df_conso.values[train_set,:]] + [c[train_set,:] for c in condarray]\n",
    "output_cvae = df_conso.values[train_set,:]\n",
    "\n",
    "new_cae.train(input_cvae, output_cvae, epochs= 2000, verbose=0,\n",
    "              batch_size=batch_size, callbacks=[earl_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer l'importance et la qualité des embeggings appris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons voir:\n",
    "- l'importance portée par des variables dans la représentation latente\n",
    "- l'importance portée par des variables dans l'embedding conditionnel\n",
    "\n",
    "Nous nous appuyons pour cela sur des score d'information, de démêlement, de compacité et de modularité\n",
    "\n",
    "Nous devrions constater que des variables importante dans l'embedding conditionnel ne le sont plus dans la représentation latente: elles ont été factorisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_encoded = new_cae.encoder.predict(input_cvae)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval, importanceMatrix =evaluate_latent_code(x_encoded, factorMatrix[train_set,:], factorDesc, orthogonalize=True, normalize_information=True)\n",
    "#normalize_information normalise le score avec le minimum obtenu avec une projection aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_evaluation_latent_code(model_eval, z_dim, factorDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_cvae = [df_conso.values[test_set,:]] + [c[test_set,:] for c in condarray]\n",
    "x_hat = new_cae.model.predict(test_input_cvae)\n",
    "\n",
    "print(f\"reconstruction error to scale : {np.mean(np.abs(scaler_conso.inverse_transform(x_hat).ravel() - df_data.Consommation.values[df_data.utc_datetime.dt.year == 2019]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prez_2D(x_encoded, factorMatrix[train_set,:], temperatureMean[train_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_emb = new_cae.cond_embedding.predict(input_cvae[1:])\n",
    "model_eval, importanceMatrix =evaluate_latent_code(cond_emb, factorMatrix[train_set,:], factorDesc, orthogonalize=True, normalize_information=True)\n",
    "#normalize_information normalise le score avec le minimum obtenu avec une projection aléatoire\n",
    "display_evaluation_latent_code(model_eval, cond_emb.shape[1], factorDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('vae_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d49fac3f4c56d943c62cc1d14e687f58a1c6af3dd65030893b7b6bcc7dc1d90f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
